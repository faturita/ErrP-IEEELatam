@article{Bauer2015,
abstract = {Restorative brain-computer interfaces (BCI) are increasingly used to provide feedback of neuronal states in a bid to normalize pathological brain activity and achieve behavioral gains. However, patients and healthy subjects alike often show a large variability, or even inability, of brain self-regulation for BCI control, known as BCI illiteracy. Although current co-adaptive algorithms are powerful for assistive BCIs, their inherent class switching clashes with the operant conditioning goal of restorative BCIs. Moreover, due to the treatment rationale, the classifier of restorative BCIs usually has a constrained feature space, thus limiting the possibility of classifier adaptation. In this context, we applied a Bayesian model of neurofeedback and reinforcement learning for different threshold selection strategies to study the impact of threshold adaptation of a linear classifier on optimizing restorative BCIs. For each feedback iteration, we first determined the thresholds that result in minimal action entropy and maximal instructional efficiency. We then used the resulting vector for the simulation of continuous threshold adaptation. We could thus show that threshold adaptation can improve reinforcement learning, particularly in cases of BCI illiteracy. Finally, on the basis of information-theory, we provided an explanation for the achieved benefits of adaptive threshold setting.},
author = {Bauer, Robert and Gharabaghi, Alireza},
doi = {10.3389/fnins.2015.00036},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {Brain-computer interface,Brain-machine interface,Brain-robot interface,Classification accuracy,Functional restoration,Neurofeedback,Neurorehabilitation,Reinforcement learning},
month = {feb},
number = {FEB},
pages = {36},
publisher = {Frontiers},
title = {{Reinforcement learning for adaptive threshold control of restorative brain-computer interfaces: A Bayesian simulation}},
url = {http://journal.frontiersin.org/Article/10.3389/fnins.2015.00036/abstract},
volume = {9},
year = {2015}
}


@incollection{Rubin2012,
abstract = {Interactions between an organism and its environment are commonly treated in the framework of Markov Decision Processes (MDP). While standard MDP is aimed solely at maximizing expected future rewards (value), the circular flow of information between the agent and its environment is generally ignored. In particular, the information gained from the environment by means of perception and the information involved in the process of action selection (i.e., control) are not treated in the standard MDP setting. In this paper, we focus on the control information and show how it can be combined with the reward measure in a unified way. Both of these measures satisfy the familiar Bellman recursive equations, and their linear combination (the free-energy) provides an interesting new optimization criterion. The tradeoff between value and information, explored using our INFO-RL algorithm, provides a principled justification for stochastic (soft) policies. We use computational learning theory to show that these optimal policies are also robust to uncertainties in settings with only partial knowledge of the MDP parameters. {\textcopyright} Springer-Verlag Berlin Heidelberg 2012.},
author = {Rubin, Jonathan and Shamir, Ohad and Tishby, Naftali},
booktitle = {Intelligent Systems Reference Library},
doi = {10.1007/978-3-642-24647-0_3},
isbn = {9783642246463},
issn = {18684394},
mendeley-groups = {ErrP},
pages = {57--74},
publisher = {Springer, Berlin, Heidelberg},
title = {{Trading value and information in MDPs}},
url = {http://link.springer.com/10.1007/978-3-642-24647-0{\_}3},
volume = {28},
year = {2012}
}




@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@incollection{kober2018bci,
  title={BCI and Games: Playful, Experience-Oriented Learning by Vivid Feedback?},
  author={Kober, Silvia E and Ninaus, Manuel and Friedrich, Elisabeth VC and Scherer, Reinhold},
  booktitle={Brain--Computer Interfaces Handbook},
  pages={209--234},
  year={2018},
  publisher={CRC Press}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{tensorflow2015-whitepaper,
title={{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}


@misc{openai,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}


@article{Watkins1992,
abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states. This paper presents and proves in detail a convergence theorem forQ-learning based on that outlined in Watkins (1989). We show thatQ-learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where manyQ values can be changed each iteration, rather than just one.},
author = {Watkins, Christopher J. C. H. and Dayan, Peter},
doi = {10.1007/bf00992698},
file = {:Users/rramele/Library/Application Support/Mendeley Desktop/Downloaded/Watkins, Dayan - 1992 - Q-learning.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
mendeley-groups = {ErrP},
month = {may},
number = {3-4},
pages = {279--292},
publisher = {Kluwer Academic Publishers},
title = {{Q-learning}},
url = {http://link.springer.com/10.1007/BF00992698},
volume = {8},
year = {1992}
}



@article{Lotte2018,
abstract = {OBJECTIVE Most current electroencephalography (EEG)-based brain-computer interfaces (BCIs) are based on machine learning algorithms. There is a large diversity of classifier types that are used in this field, as described in our 2007 review paper. Now, approximately ten years after this review publication, many new algorithms have been developed and tested to classify EEG signals in BCIs. The time is therefore ripe for an updated review of EEG classification algorithms for BCIs. APPROACH We surveyed the BCI and machine learning literature from 2007 to 2017 to identify the new classification approaches that have been investigated to design BCIs. We synthesize these studies in order to present such algorithms, to report how they were used for BCIs, what were the outcomes, and to identify their pros and cons. MAIN RESULTS We found that the recently designed classification algorithms for EEG-based BCIs can be divided into four main categories: adaptive classifiers, matrix and tensor classifiers, transfer learning and deep learning, plus a few other miscellaneous classifiers. Among these, adaptive classifiers were demonstrated to be generally superior to static ones, even with unsupervised adaptation. Transfer learning can also prove useful although the benefits of transfer learning remain unpredictable. Riemannian geometry-based methods have reached state-of-the-art performances on multiple BCI problems and deserve to be explored more thoroughly, along with tensor-based methods. Shrinkage linear discriminant analysis and random forests also appear particularly useful for small training samples settings. On the other hand, deep learning methods have not yet shown convincing improvement over state-of-the-art BCI methods. SIGNIFICANCE This paper provides a comprehensive overview of the modern classification algorithms used in EEG-based BCIs, presents the principles of these methods and guidelines on when and how to use them. It also identifies a number of challenges to further advance EEG classification in BCI.},
author = {Lotte, F. and Bougrain, L. and Cichocki, A. and Clerc, M. and Congedo, M. and Rakotomamonjy, A. and Yger, F.},
doi = {10.1088/1741-2552/aab2f2},
file = {:Users/rramele/Library/Application Support/Mendeley Desktop/Downloaded/Lotte et al. - 2018 - A review of classification algorithms for EEG-based brainâ€“computer interfaces a 10 year update.pdf:pdf},
isbn = {1741-2560 (Print) 1741-2552 (Linking)},
issn = {17412552},
journal = {Journal of Neural Engineering},
keywords = {Riemannian geometry,brain-computer interfaces,classification,deep learning,electroencephalography,spatial filtering,transfer learning},
mendeley-groups = {Thesis,Histogram of Oriented Gradients P300},
month = {jun},
number = {3},
pages = {031005},
pmid = {29488902},
publisher = {IOP Publishing},
title = {{A review of classification algorithms for EEG-based brain-computer interfaces: A 10 year update}},
url = {http://stacks.iop.org/1741-2552/15/i=3/a=031005?key=crossref.9cd2b15ab65c8ad34b475584b43dc509},
volume = {15},
year = {2018}
}



@inproceedings{Omedes2013,
abstract = {EEG brain-computer interfaces (BCI) require a calibration phase prior to the on-line control of the device, which is a difficulty for the practical development of this technology as it is user-, session- and task-specific. The large body of research in BCIs based on event-related potentials (ERP) use temporal features, which have demonstrated to be stable for each user along time, but do not generalize well among tasks different from the calibration task. This paper explores the use of low frequency features to improve the generalization capabilities of the BCIs using error-potentials. The results show that there exists a stable pattern in the frequency domain that allows a classifier to generalize among the tasks. Furthermore, the study also shows that it is possible to combine temporal and frequency features to obtain the best of both domains. {\textcopyright} 2013 IEEE.},
author = {Omedes, Jason and Iturrate, Inaki and Montesano, Luis and Minguez, Javier},
booktitle = {Proceedings of the Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBS},
doi = {10.1109/EMBC.2013.6610736},
isbn = {9781457702167},
issn = {1557170X},
mendeley-groups = {ErrP},
month = {jul},
pages = {5263--5266},
publisher = {IEEE},
title = {{Using frequency-domain features for the generalization of EEG error-related potentials among different tasks}},
url = {http://ieeexplore.ieee.org/document/6610736/},
year = {2013}
}


@incollection{Plass-OudeBos2010,
abstract = {Recently research into Brain-Computer Interfacing (BCI) applications for healthy users, such as games, has been initiated. But why would a healthy person use a still-unproven technology such as BCI for game interaction? BCI provides a combination of information and features that no other input modality can offer. But for general acceptance of this technology, usability and user experience will need to be taken into account when designing such systems. Therefore, this chapter gives an overview of the state of the art of BCI in games and discusses the consequences of applying knowledge from Human-Computer Interaction (HCI) to the design of BCI for games. The integration of HCI with BCI is illustrated by research examples and showcases, intended to take this promising technology out of the lab. Future research needs to move beyond feasibility tests, to prove that BCI is also applicable in realistic, real-world settings.},
author = {{Plass-Oude Bos}, Danny and Reuderink, Boris and van de Laar, Bram and G{\"{u}}rk{\"{o}}k, Hayrettin and M{\"{u}}hl, Christian and Poel, Mannes and Nijholt, Anton and Heylen, Dirk},
doi = {10.1007/978-1-84996-272-8_10},
mendeley-groups = {ErrP},
pages = {149--178},
publisher = {Springer, London},
title = {{Brain-Computer Interfacing and Games}},
url = {http://link.springer.com/10.1007/978-1-84996-272-8{\_}10},
year = {2010}
}


@inproceedings{Schiatti2018,
abstract = {Shared control and shared autonomy play an important role in assistive technologies, allowing the offloading of the cognitive burden required for control from the user to the intelligent robotic device. In this context, electrophysiological measures of error detection, directly measured from a person's brain activity as Error-related Potentials (ErrPs), can be exploited to provide passive adaptation of an external semi-autonomous system to the human. This concept was implemented in an online robot learning task, where user's evaluation of the robot's actions, in terms of detected ErrP, was exploited to update a reward function in a Reinforcement Learning (RL) framework. Results from both simulated and experimental studies show that the introduction of human evaluation in the robot learning loop allows for: (1) the acceleration of optimal policy learning in a target reaching task, (2) the introduction of a further degree of control in robot learning, namely identification of one among multiple targets, according to the user's will. Overall, presented results support the potential of human-robot co-adaptive and co-operative strategies to develop human-centered assistive technologies.},
author = {Schiatti, Lucia and Tessadori, Jacopo and Deshpande, Nikhil and Barresi, Giacinto and King, Louis C. and Mattos, Leonardo S.},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2018.8460551},
isbn = {9781538630815},
issn = {10504729},
mendeley-groups = {ErrP},
month = {may},
pages = {4473--4480},
publisher = {IEEE},
title = {{Human in the Loop of Robot Learning: EEG-Based Reward Signal for Target Identification and Reaching Task}},
url = {https://ieeexplore.ieee.org/document/8460551/},
year = {2018}
}


@article{Holroyd2009,
abstract = {A recent theory holds that the anterior cingulate cortex (ACC) uses reinforcement learning signals conveyed by the midbrain dopamine system to facilitate flexible action selection. According to this position, the impact of reward prediction error signals on ACC modulates the amplitude of a component of the event-related brain potential called the error-related negativity (ERN). The theory predicts that ERN amplitude is monotonically related to the expectedness of the event: It is larger for unexpected outcomes than for expected outcomes. However, a recent failure to confirm this prediction has called the theory into question. In the present article, we investigated this discrepancy in three trial-and-error learning experiments. All three experiments provided support for the theory, but the effect sizes were largest when an optimal response strategy could actually be learned. This observation suggests that ACC utilizes dopamine reward prediction error signals for adaptive decision making when the optimal behavior is, in fact, learnable. {\textcopyright} 2009 The Psychonomic Society, Inc.},
author = {Holroyd, Clay B. and Krigolson, Olave E. and Baker, Robert and Lee, Seung and Gibson, Jessica},
doi = {10.3758/CABN.9.1.59},
file = {:Users/rramele/Library/Application Support/Mendeley Desktop/Downloaded/Holroyd et al. - 2009 - When is an error not a prediction error An electrophysiological investigation.pdf:pdf},
issn = {15307026},
journal = {Cognitive, Affective and Behavioral Neuroscience},
mendeley-groups = {ErrP},
month = {mar},
number = {1},
pages = {59--70},
publisher = {Springer-Verlag},
title = {{When is an error not a prediction error? An electrophysiological investigation}},
url = {http://www.springerlink.com/index/10.3758/CABN.9.1.59},
volume = {9},
year = {2009}
}


@article{Renard2010,
author = {Renard, Yann and Lotte, Fabien and Gibert, Guillaume and Congedo, Marco and Maby, Emmanuel and Delannoy, Vincent and Bertrand, Olivier and L{\'{e}}cuyer, Anatole},
doi = {10.1162/pres.19.1.35},
issn = {1054-7460},
journal = {Presence: Teleoperators and Virtual Environments},
mendeley-groups = {BCI,Histogram of Oriented Gradients P300},
month = {feb},
number = {1},
pages = {35--53},
title = {{OpenViBE: An Open-Source Software Platform to Design, Test, and Use Brainâ€“Computer Interfaces in Real and Virtual Environments}},
url = {http://www.mitpressjournals.org/doi/abs/10.1162/pres.19.1.35},
volume = {19},
year = {2010}
}


@article{Chavarriaga2014,
abstract = {The ability to recognize errors is crucial for efficient behavior. Numerous studies have identified electrophysiological correlates of error recognition in the human brain (error-related potentials, ErrPs). Consequently, it has been proposed to use these signals to improve human-computer interaction (HCI) or brain-machine interfacing (BMI). Here, we present a review of over a decade of developments toward this goal. This body of work provides consistent evidence that ErrPs can be successfully detected on a single-trial basis, and that they can be effectively used in both HCI and BMI applications. We first describe the ErrP phenomenon and follow up with an analysis of different strategies to increase the robustness of a system by incorporating single-trial ErrP recognition, either by correcting the machine's actions or by providing means for its error-based adaptation. These approaches can be applied both when the user employs traditional HCI input devices or in combination with another BMI channel. Finally, we discuss the current challenges that have to be overcome in order to fully integrate ErrPs into practical applications. This includes, in particular, the characterization of such signals during real(istic) applications, as well as the possibility of extracting richer information from them, going beyond the time-locked decoding that dominates current approaches. {\textcopyright} 2014 Chavarriaga, Sobolewski and Mill{\'{a}}n.},
author = {Chavarriaga, Ricardo and Sobolewski, Aleksander and Mill{\'{a}}n, Jos{\'{e}} del R.},
doi = {10.3389/fnins.2014.00208},
file = {:Users/rramele/Library/Application Support/Mendeley Desktop/Downloaded/Chavarriaga, Sobolewski, Mill{\~{A}}Â¡n - 2014 - Errare machinale est the use of error-related potentials in brain-machine interfaces.pdf:pdf},
issn = {1662453X},
journal = {Frontiers in Neuroscience},
keywords = {Brain-machine interface,EEG,Error-related potentials,Hybrid BCI,Neuroprosthesis,Reinforcement learning},
month = {jul},
number = {8 JUL},
pages = {208},
publisher = {Frontiers},
title = {{Errare machinale est: The use of error-related potentials in brain-machine interfaces}},
url = {http://journal.frontiersin.org/article/10.3389/fnins.2014.00208/abstract},
volume = {8},
year = {2014}
}



@book{AIAMA,
  title={Artificial Intelligence: A Modern Approach},
  author={Norvig, Peter and Russell, Stuart},
  year={2009},
  publisher={Prentice Hall Press Upper Saddle River}
}

@book{REINFORCEMENT-LEARNING-BOOK,
  title={Reinforcement Learning: An Introduction},
  author={Andrew Barto, Richard S. Sutton},
  year={2018},
  publisher={Cambridge, MA : The MIT Press}
}

@article{Santos1999,
abstract = {The aim of this work is to present a method that helps tune the reinforcement function parameters in a reinforcement learning approach. Since the proposal of neural-based implementations for the reinforcement learning paradigm (which reduced learning time and memory requirements to realistic values) reinforcement functions have become the critical components. Using a particular definition for reinforcement functions (RF), we solve, for a specific case, the so-called exploration versus exploitation dilemma through the careful computation of the RF parameter values. The proposed algorithm computes, during the exploration part of the learning phase, an estimate for the parameter values. Experiments with the mobile robot Nomad 200 validate our proposals.},
author = {Santos, Juan Miguel and Touzet, Claude},
doi = {10.1016/S0925-2312(98)00117-9},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Autonomous robot,Behavior-based approach,Reinforcement function,Reinforcement learning,Robot learning},
mendeley-groups = {ErrP},
month = {oct},
number = {1-3},
pages = {93--105},
publisher = {Elsevier},
title = {{Exploration tuned reinforcement function}},
url = {https://www.sciencedirect.com/science/article/pii/S0925231298001179},
volume = {28},
year = {1999}
}


@article{ALPHA-GO,
  title={Mastering the Game of Go without Human Knowledge},
  author={David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, Demis Hassabis.},
  year={2017},
}

@article{EERP-PAPER,
  title={Error-related EEG Potentials In Brain-Computer Interfaces},
  author={Ferrez, Pierre},
  year={2007},
}

@article{TRANSFER-LEARNING,
  title={Transfer Learning in Brain-Computer Interfaces},
  author={Jayaram, Vinay},
  year={2015},
}

@article{RAMELE,
  title={Histogram of Gradient Orientations of EEG Signal Plots for Brain Computer Interfaces},
  author={Ramele, Rodrigo},
  year={2018},
}

@article{ERRP-GAME-PAPER,
  title={Error-related potentials during continuous feedback: using EEG to detect errors of different type and severity},
  author={Sp\"{u}ler, Martin and Niethammer, Christian},
  year={2015},
  doi={10.3389/fnhum.2015.00155},
  url={https://www.researchgate.net/publication/273278100_Error-related_potentials_during_continuous_feedback_Using_EEG_to_detect_errors_of_different_type_and_severity}
}

@article{ERRP-DATASET,
  title={Dataset description: Error-related potentials (ErrPs) during continuous feedback},
  author={Sp\"{u}ller, Martin and Niethammer, Christian},
  year={2015},
  url={https://manualzz.com/doc/40119616/dataset-description--error-related-potentials--errps--during}
}

@book{BCI-BOOK,
    title={Brain-Computer Interfaces: Principles and Practice},
    author={Jonathan Wolpaw and Elizabeth Winter Wolpaw},
    year={2012},
    publisher={Oxford University Press},
    keywords="Brain Computer Interface research, BCI, brain activity, central nervous system, brain signals, severely disabled, muscle control, communication, technology"
}

@book{CURSOR-CONTROL-PAPER,
    title={Neuroadaptive technology enables implicit cursor control based on medial prefrontal cortex activity},
    author={Thorsten O. Zander, Laurens R. Krol, Niels P. Birbaumer and Klaus Gramann},
    year={2016},
    publisher={},
    keywords=""
}

@article{ROBOT-CONTROL-PAPER,
    title={Robot Reinforcement Learning using EEG-based reward signals},
    author={I. Iturrate, L. Montesano, J. Minguez},
    year={2010},
    publisher={},
    keywords=""
}

@article{ERP-UPS_AND_DOWNS,
    title={ERP Components: The Ups and Downs of Brainwave Recordings},
    author={Emily S. Kappenman and Steven J. Luck},
    year={2011},
    keywords="ERP"
}

@article{EEG-DEF-PAPER,
    title={Analysis of Electroencephalography (EEG) Signals and Its Categorizationâ€“A Study},
    author={J. Satheesh Kumar and P. Bhuvaneswari},
    year={2012},
    keywords="EEG"
}

@article{OPEN-VIBE-PAPER,
    title={OpenViBE: An Open-Source Software Platform to Design, Test and Use Brain-Computer Interfaces in Real and Virtual Environments},
    author={Yann Renard, Fabien Lotte, Guillaume Gibert, Marco Congedo, Emmanuel Maby, Vincent Delannoy, Olivier Bertrand, and Anatole Lecuyer},
    year={2010},
    keywords=""
}

% @article{pearson1931test,
%   title={The test of significance for the correlation coefficient},
%   author={Pearson, Egon S},
%   journal={Journal of the American Statistical Association},
%   volume={26},
%   number={174},
%   pages={128--134},
%   year={1931},
%   publisher={Taylor \& Francis}
% }

@Manual{MNE,
    title = {MNE software for processing MEG and EEG data, NeuroImage, Volume 86},
    author = {A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, L. Parkkonen, M. H\"{a}lm\"{a}ll\"{a}linen},
    year = {2014},
    url = {https://mne-tools.github.io/0.13/index.html}
  }
  
@Manual{MNE-PYTHON,
    title = {MEG and EEG data analysis with MNE-Python, Frontiers in Neuroscience, Volume 7},
    author = {A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier, C. Brodbeck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, M. H\"{a}lm\"{a}ll\"{a}linen},
    year = {2013},
    url = {https://mne-tools.github.io/0.13/index.html}
  }

