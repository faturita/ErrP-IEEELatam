\documentclass[journal,onecolumn,12pt]{IEEEtran} 

\usepackage{amsmath,amssymb,bm}
\usepackage{amsthm, amsfonts}	
\usepackage{bm,bbm}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{fancybox}
\usepackage{url,booktabs}
\usepackage[round]{natbib}

\usepackage{xr}



\title{Reply to Reviewer's Comments on\\
``Training a gaming agent on brainwaves''}
\author{}

\begin{document}

\maketitle
\pagenumbering{roman}
\setcounter{page}{1}

We are grateful to the reviewer for pointing out relevant issues in our manuscript.

In the following, we discuss how we dealt with each raised issue. 

\vskip+1ex
\noindent \dotfill

\section*{\fbox{Reviewer \#1 Transcript:}}

This paper proposes the use of a gaming agent to be trained using Reinforcement Learning (RL) as well as the feedback obtained from EEG brainwaves of human critic observers. The idea of using brain patterns for boosting ML is highly interesting and well suited for the journal. Although it is not a new concept, it is nicely re-purposed for online use-case.

However, I am not sure if there is something new in the paper compared to the 2010 proof-of-concept (Iturrate et al.: Robot Reinforcement Learning using EEG-based reward signals). The presentation style seems a bit confusing and needs serious improvements. Background is very limited (some are incorporated in the introduction) and substantial work is required. There has been a lot of work in BCIs over the past decade and some in the area of games.

From the scientific part, the results are not so convincing. In particular, the proposed idea does not seem to be too practical method. The classification does not provide something significant as it stands at the moment. The sample used is also quite small and the complexity of the game is limited.

Nevertheless, the paper could be improved in many ways. An obvious point would be to improve the classifiers. Apart from this, I would suggest that authors would look at ways of manipulating the stimulus, stimulus presentation or giving out incentives to participants.


\section*{\fbox{Reviewer \#2 Transcript:}}

The paper is interesting and relevant. However in my opinion it needs revision for purposes of clarification and to improve the contribution.
Please see attached document for my comments.

Training a Gaming Agent on Brainwaves
There are too many grammatical errors to list individually.
//Does the header refer to a general template?
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
//The abstract read more like an introduction, rather than an indication of the work undertaken for the paper.
25 “Results show that there is an effective transfer of information and that
the agent learns successfully to solve the game efficiently.”
//This is too vague. What are the outcomes of the study?
39 This information is used to make a gaming agent improves its operational performance using electroencephalography (EEG) signals as feedback of the performed task, obtained from an observational human critic.
//There is a grammar issue. I don’t understand this sentence.
54 RL should be defined in main body of the paper. Also the term ‘agent’ needs to be defined/clarified. How does it relate to the Game Manager from Figure 1?
17 col 2 Recently, this technique has seen a come-back. //this is not scientific language
58 col 3 The precision of 25.125 is inappropriate P2, 46 col 2 What is “state information”?
P3, 2 red normally indicates an error
P3, 11 I assume that the “observational human critic” is the player/participant/subject/human observer. This role should be clarified and terminology used consistently.
P3, 41 define MNE
P3, 56 Thus, each epoch is composed of a matrix 500 x 8. // add channels
P4, 32 Hence, following the iterative procedure based on Equation 1, the Q-Table is updated in each iteration. After the algorithm finishes iterating through all the training episodes, the Q-Table is stored to test the performance of the agent.
// Will the game always terminate? How long does the game take? Does smooth progression toward finish affect the err potential?
P4 Fig 3. Is the chance score = 0.5?
P4 the labels referred to in Fig 5 should correlate with the test here, i.e., A= subject 1 etc.
P4 referring to Fig 6; a comparison ROC curve with subject 1 would be more interesting

P5, 34 what is meant by “experiences”?
P5, 29 col 2 – remove Average steps per Q-Table legend.
P5 50, col 2 The collected data show that ErrP signals can in fact be classified and used to train an agent effectively.
// how has effectiveness been determined here?
Page 6 Fig 10 the text is not legible, it should be improved.
Page 6, 40
“However, even though this implies that the agent misses frequently that an action taken is wrong, this is not hindering the overall performance and the agent is still learning.”
// Your results show that this is subject dependent
P6, 56
Results show that training a classifier with data of one subject, but using it
to classify the events of experiences of another subject does
not lead to an improvement on the performance of the agent.
// could a pre-trained generic classifier provide a better initial state, subsequently trained with observer data to converge more quickly?
Are there differential err potential for up, down, left, right?

\section*{\fbox{Reviewer \#3 Transcript:}}

Comments to the Author
The following work shows the usage of ErrPs for training a gaming agent using reinforcement learning. Authors attempt different conventional classification approaches, as well as intersubject classification.

It is unclear to me the benefit of evaluating single subject to single subject offline classification accuracy. Please elaborate on this. Since different subjects attained different performance, all combinations of subjects used for training and testing should be inspected for the 1:1 evaluation setup. Maybe classification accuracy could be reported at different steps (depending on the amount of training data – gradual increasing the number of subjects)
What is the ratio of hit/no hit segments of action (epochs)?
Page 3, column 2, lines 4-9. It is unclear at this stage what action from the starting point would generate an ErrP.
moves-> move
Page 3, column 2, lines 11-13. Has the MinMax Scaler been applied in any other BCI related study or elsewhere? I think including a reference would be useful.
Page 4. Figure 3. I would suggest adding another set of bars for the grand average classification scores.
Page 4, Figure 4. This figure is redundant and could be removed.
Page 5, Figure 5. The titles of the subplots would be more descriptive when replacing the letters with the corresponding subjects number.
Page 5, column 1, line 52, Not ->no
Page 5, column1, line 55, accumulative->cumulative
Page 6, Figure 8. It is unclear to me what is the benefit of showing both A and B. Since the behavior is similar in both cases.
Page 6, Figure 10. This figure is, in my opinion, incomplete. What is the reason for showing this subset of subjects? I suggest showing the confusion matrices of all subjects and/or their average.
Page 6, column 2, line 40. show -> shows

\subsection*{\ovalbox{Reviewer 1 General Comments}}

This paper proposes the use of a gaming agent to be trained using Reinforcement Learning (RL) as well as the feedback obtained from EEG brainwaves of human critic observers. The idea of using brain patterns for boosting ML is highly interesting and well suited for the journal. Although it is not a new concept, it is nicely re-purposed for online use-case.

However, I am not sure if there is something new in the paper compared to the 2010 proof-of-concept (Iturrate et al.: Robot Reinforcement Learning using EEG-based reward signals). The presentation style seems a bit confusing and needs serious improvements. Background is very limited (some are incorporated in the introduction) and substantial work is required. There has been a lot of work in BCIs over the past decade and some in the area of games.

\begin{quotation}
{\color{blue}
We conducted a new BCI and games literature review and updated the introduction accordingly, adding more updated background.  We highlighted what we think is our contribution, which is a simple game that can be used to trigger the ErrP response in a simple scenario that can be used to train an agent.  We were unable to find a similar work from that same perspective.  
}
\end{quotation}


From the scientific part, the results are not so convincing. In particular, the proposed idea does not seem to be too practical method. The classification does not provide something significant as it stands at the moment. The sample used is also quite small and the complexity of the game is limited.

\begin{quotation}
{\color{blue}
Issues raised by the Reviewer are accurate and timely described.  We aimed to find a suitable game that at the same time can be trained with a Reinforcement Learning technique and that triggered the ErrP response as well.  We modified the manuscript, emphasized three results of our experiments.  First we show that the basic Q-Learning algorithm is robust to noisy signals.  Additionally,  we showed the futility of transfer learning approaches using the brainwaves signals, and finally that the cumulative contribution of the rewards obtained from different subjects enhances the performance of an agent.
}
\end{quotation}


Nevertheless, the paper could be improved in many ways. An obvious point would be to improve the classifiers. Apart from this, I would suggest that authors would look at ways of manipulating the stimulus, stimulus presentation or giving out incentives to participants.

\begin{quotation}
{\color{blue}
We rewrote the classification scheme and performed two more experiments.  First we included kNeighbours as a new classification algorithm.  This can be seen in Section B and in Figure 3.  Moreover, we performed a complete transfer learning experiment, verifying if it was possible to improve the efficiency of the gaming agent  and verified that the best results for the improvement (i.e. reduction) on the average number of steps required to reach the target in 200 plays is achieved when a classifier is trained with ErrP signals from the same subject that is used to identify the ErrP and to provide the rewards.    This is added in Section V and in Figure 7.
}
\end{quotation}

\subsection*{\ovalbox{Reviewer 2 General Comments}}
The paper is interesting and relevant. However in my opinion it needs revision for purposes of clarification and to improve the contribution.
Please see attached document for my comments.

Training a Gaming Agent on Brainwaves
There are too many grammatical errors to list individually.

\begin{quotation}
{\color{blue}
We truly appreciate this level of detail in all the comments and the requests for information.  The value provided by the comments in huge.  Thanks !!
}
\end{quotation}


//Does the header refer to a general template?
JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

\begin{quotation}
{\color{blue}
This is a shameful mistake, we are terrible sorry.  We thought this was going to be replaced by the platform automatically. Thanks for pointing out this error.
}
\end{quotation}

//The abstract read more like an introduction, rather than an indication of the work undertaken for the paper.
25 “Results show that there is an effective transfer of information and that
the agent learns successfully to solve the game efficiently.”
//This is too vague. What are the outcomes of the study?

\begin{quotation}
{\color{blue}
We performed several changes in the manuscript.  We modified the introduction describing those changes and emphasizing what we think may be our main contributions.
}
\end{quotation}

39 This information is used to make a gaming agent improves its operational performance using electroencephalography (EEG) signals as feedback of the performed task, obtained from an observational human critic.
//There is a grammar issue. I don’t understand this sentence.

\begin{quotation}
{\color{blue}
We modified the sentence.  We thanks the reviewer for pointing out this issue.
}
\end{quotation}

54 RL should be defined in main body of the paper. Also the term ‘agent’ needs to be defined/clarified. How does it relate to the Game Manager from Figure 1?

\begin{quotation}
{\color{blue}
The acronym was revised and we also verified all the other acronyms used in the text.  The term "agent" was defined, and their relation with the Game Manager in Figure 1 was established.
}
\end{quotation}

17 col 2 Recently, this technique has seen a come-back. // this is not scientific language

\begin{quotation}
{\color{blue}
This mistake has been corrected.  We apologize for the bad choice of words.
}
\end{quotation}

58 col 3 The precision of 25.125 is inappropriate P2, 46 col 2 What is “state information”?

\begin{quotation}
{\color{blue}
The precision was adjusted to two decimals for the age mean and the standard deviation.  We removed  "state information" and replaced the wording of the phrase in order to clarify more clearly the information that we wanted to convey:  we refer to state information to the sequence of movement that the gaming performs on the game.
}
\end{quotation}

P3, 2 red normally indicates an error

\begin{quotation}
{\color{blue}
We understand the Reviewer's point, and we agree with her/him that red is not the right color to represent the completion of a task in the game, and in this case it may have cognitive implications regarding the generalized concept that red indicates an error.   It is known that the ErrP response may be affected by colors and shapes~\cite{EIMER1997143}.  The experiments that we performed were implemented with the board showed on Figure 2 of the manuscript.  We added this very important issue at the conclusions in relation with possible future works.
}
\end{quotation}

P3, 11 I assume that the “observational human critic” is the player/participant/subject/human observer. This role should be clarified and terminology used consistently.

\begin{quotation}
{\color{blue}
Absolutely.  We preferred to use OHC "observational human critic" to emphasize what the subject is actually doing while using the system.
}
\end{quotation}

P3, 41 define MNE

\begin{quotation}
{\color{blue}
Historically MNE standed for Minimum Error Estimate, a software package developed in Martinos Center of Harvard University.  Now MNE  is the name of a entire software platform to perform analysis of several type of brain signals like Magnetoencephalograpy and Electroencephalography.
}
\end{quotation}

P3, 56 Thus, each epoch is composed of a matrix 500 x 8. // add channels

\begin{quotation}
{\color{blue}
Excellent.  We added that information.
}
\end{quotation}

P4, 32 Hence, following the iterative procedure based on Equation 1, the Q-Table is updated in each iteration. After the algorithm finishes iterating throughout all the training episodes, the Q-Table is stored to test the performance of the agent.
// Will the game always terminate? How long does the game take? Does smooth progression toward finish affect the err potential?

\begin{quotation}
{\color{blue}

If the gaming agent moves randomly on the board of Figure 2, it takes on average 100 steps to arrive to the final location on the grid.  Each step, the movement direction is selected from the Q-Table once every 2 seconds, so on average it takes around 200 seconds to finish the game.  After some training, the Q-Table could potentially end up in loops so if the steps count arrives to 200 the game is interrupted and it starts all over again.
So far we didn't test if the Error Potential is affected by the smooth progression toward the end, which is indeed something interesting to verify in this simple scenario.  We added this very interesting extension in Section 6.

}
\end{quotation}


P4 Fig 3. Is the chance score = 0.5?

\begin{quotation}
{\color{blue}
Yes, it is.  We added that information in the Figure's caption.
}
\end{quotation}

P4 the labels referred to in Fig 5 should correlate with the test here, i.e., A= subject 1 etc.

\begin{quotation}
{\color{blue}
The figure labels on Figure 4(now) were modified to reflect which graph references which subject.  Additionally, we use the same notation on the new Figure 5.
}
\end{quotation}

P4 referring to Fig 6; a comparison ROC curve with subject 1 would be more interesting

\begin{quotation}
{\color{blue}
We added on Figure 5(now) all the ROC curves that we obtained for all the different subjects, including the one for Subject 1.  
}
\end{quotation}


P5, 34 what is meant by “experiences”?

\begin{quotation}
{\color{blue}
Each experience is defined by.....
}
\end{quotation}

P5, 29 col 2 – remove Average steps per Q-Table legend.

\begin{quotation}
{\color{blue}
This was modified in all the Figures, adjusting fonts and labels.  We appreciate a lot the time to point out this feature to enhance the quality of the manuscript.
}
\end{quotation}

P5 50, col 2 The collected data show that ErrP signals can in fact be classified and used to train an agent effectively.
// how has effectiveness been determined here?

\begin{quotation}
{\color{blue}
Each time the gaming agent plays this simple game, it takes on average around 100 steps to reach the target spot.   We asked an observational human critic to watch the gaming agent play the game, while we recorded their brainwaves.  We trained first a classifier to be able to recognize Error Potentials from these brainwaves.  We started the game again, and the gaming agent started to move around the board, but this time each time an Error Potential was identified from the brainwaves, it was used as a negative reward to train a new Q-Table for the agent.  We performed another experience, but this time the gaming agent used the Q-Table that was updated in the previous experience.
We verified that by doing this experiment, the agent required less and less steps on average to reach the goal until it arrives to the optimal number of around 10 steps.  We found that although the number of training sessions depended on the accuracy of the classifier, even with very low values (just above chance level), the agent learns and the number of average number of steps is reduced.
We verified that if trained the agent with noise signals, completely uncorrelated with the reward, the agent learned nothing, and the average number of steps was not reduced at all.
Finally, we also verified that if we train a classifier with Error Potentials from one subject and used that classifier to provide the rewards for the experiment, the number of steps is not reduced, and that do not depend on the subjects, is an instrinsical result which is produced when mixing the clasifiers.  Only the usage of a classifier from the same subject produces this improvement.
Finally, we verified that rewards obtained from different subjects, can be used to improve the performance of the gaming agent.
}
\end{quotation}

Page 6 Fig 10 the text is not legible, it should be improved.

\begin{quotation}
{\color{blue}
This is now Figure 9, we increased the fontsize, added a colormap and included the confusion matrices for all the subjects.
}
\end{quotation}

Page 6, 40
“However, even though this implies that the agent misses frequently that an action taken is wrong, this is not hindering the overall performance and the agent is still learning.”
// Your results show that this is subject dependent

\begin{quotation}
{\color{blue}
Yes, the Reviewer is correct.  We modified this sentence to state more clearly the message that we wanted to convey, which is, that results are completely subject dependent, but that 
}
\end{quotation}

P6, 56
Results show that training a classifier with data of one subject, but using it
to classify the events of experiences of another subject does
not lead to an improvement on the performance of the agent.

// could a pre-trained generic classifier provide a better initial state, subsequently trained with observer data to converge more quickly?
Are there differential err potential for up, down, left, right?

\begin{quotation}
{\color{blue}
The statement is absolutely right.  We extended this experience per Reviewer's suggestion and we verified that effectively the only combination of training/testing in the recognition of ErrP that produces an improvement on the performance of the agent is the one that uses information from the same subject (check new Figure 7).   We think a generic-clasiffier to identify ErrP potentials for any subject (i.e. without calibration) is a very research goal of the BCI community because it will allow to have more robust, and easier to use, with shorter setups, BCI devices.  If such a generic pre-trained classifier could be conceived, this simple game can be used to verify if the number of training sessions required is reduced and this will confirm that the pre-trained classifier is helping to do it more quickly.  We didn't verify if the error potential for up down left right per se produces a different error potential, though we verified that in general getting farther from the target (in Manhattan distance) produces a signal which is slightly different from the one that is produced when the gaming agent moves closer to the target.  We added these very interesting experiences to perform in Section 6.
}
\end{quotation}


\vskip+1ex
\noindent \dotfill
\vskip+1ex

\subsection*{\ovalbox{Reviewer 3 General Comments}}

Comments to the Author
The following work shows the usage of ErrPs for training a gaming agent using reinforcement learning. Authors attempt different conventional classification approaches, as well as intersubject classification.

\begin{quotation}
{\color{blue}
We appreciate a lot the time dedicated to review our manuscript.
}
\end{quotation}


It is unclear to me the benefit of evaluating single subject to single subject offline classification accuracy. Please elaborate on this. 

\begin{quotation}
{\color{blue}
We tested many variants of simple games and we found that only the one proposed here produced a distinctive  ErrP response. The single subject accuracy that is presented on Figure 5(now) shows the level of ErrP single trial identification that we achieved for different subjects (which is low).  We wanted to emphasize the finding that even with such low levels of identification, it was possible for the agent to learn an efficient strategy.
}
\end{quotation}

Since different subjects attained different performance, all combinations of subjects used for training and testing should be inspected for the 1:1 evaluation setup. 

\begin{quotation}
{\color{blue}
This is an excellent idea that we now explored with a new experiment.  We showed the results on Figure 7, where a classifier is trained to recognize ErrP signals using the information obtained from one Trainer subject and used to generate rewards for a gaming agent on a different Tester subject.  We added information about this on Section 5.
}
\end{quotation}

Maybe classification accuracy could be reported at different steps (depending on the amount of training data – gradual increasing the number of subjects)

\begin{quotation}
{\color{blue}
The classification accuracy obtained for all subject is now being shown on Figure 5 with ROC curves.  What is shown progressively is the experiment itself, where after each training session, a new Q-Table is generated based on rewards obtained from the previous training session and the agent improves their efficiency by reaching the target in a progressively less number of steps on average.  
}
\end{quotation}


What is the ratio of hit/no hit segments of action (epochs)?


\begin{quotation}
{\color{blue}
The initial ratio hit/no-hit segments is 50/50, so for each training session, as the average number of steps is around 100, it will be 50 segments of hit (moving further from the target) or no-hit (moving closer).  As the Q-Table is being updated, the number of hit segments slightly decreases.
}
\end{quotation}


Page 3, column 2, lines 4-9. It is unclear at this stage what action from the starting point would generate an ErrP.
moves-> move

\begin{quotation}
{\color{blue}
The Reviewer is absolutely right in that there isn't any possible action that the gaming agent can perform at the beginning of the experience (the game agent is located at the upper-left corner).  We modified the phrase to emphasize the message that we wanted to convey, and that there is a very important caveat of the experiment in terms that the gaming agent always has a 5\% chance of wandering and selecting an action randomly, regardless of their Q-Train (whether it is updated or not).  This is precisely to avoid loops or deadlocks situations.
On the other hand, we fixed the pointed out grammatical error as well.
}
\end{quotation}


Page 3, column 2, lines 11-13. Has the MinMax Scaler been applied in any other BCI related study or elsewhere? I think including a reference would be useful.

\begin{quotation}
{\color{blue}
We added a reference where the MinMaxScaler is used in a BCI application (new page 3, second column).
}
\end{quotation}

Page 4. Figure 3. I would suggest adding another set of bars for the grand average classification scores.

\begin{quotation}
{\color{blue}
LO QUE PIDE ACA EL REVISOR ES AGREGAR UNA BARRA MAS A CADA SUJETO CON LA CLASIFICACION HACIENDO PROMEDIO DE TODAS LAS SENALES.  MMMM Cre que esto sería dificil tenerlo....
}
\end{quotation}

Page 4, Figure 4. This figure is redundant and could be removed.

\begin{quotation}
{\color{blue}
This figure was removed, thank you very much for your comment.
}
\end{quotation}


Page 5, Figure 5. The titles of the subplots would be more descriptive when replacing the letters with the corresponding subjects number.

\begin{quotation}
{\color{blue}
We agree with Reviewer's comment.  We removed the letters and replace them with subject's numbers.
}
\end{quotation}

Page 5, column 1, line 52, Not ->no

\begin{quotation}
{\color{blue}
Fixed.  Thank you very much !
}
\end{quotation}

Page 5, column1, line 55, accumulative->cumulative

\begin{quotation}
{\color{blue}
Fixed.
}
\end{quotation}


Page 6, Figure 8. It is unclear to me what is the benefit of showing both A and B. Since the behavior is similar in both cases.

\begin{quotation}
{\color{blue}
We included now all the ROC curves for all the subjects, where the difference in AUCs can be seen more clearly.
}
\end{quotation}

Page 6, Figure 10. This figure is, in my opinion, incomplete. What is the reason for showing this subset of subjects? I suggest showing the confusion matrices of all subjects and/or their average.

\begin{quotation}
{\color{blue}
Absolutely right.  We included all the confusion matrices for all the subjects.  The reason for this is that we wanted to convey the message that our hypothesis is that as false positives are low, those ErrP that are triggered are indeed good "rewards" that are useful for the training algorithm and the gaming agent is using that information, tough scarce, in a useful way to learn and improve it performance.
}
\end{quotation}

Page 6, column 2, line 40. show -> shows

\begin{quotation}
{\color{blue}
Fixed.
}
\end{quotation}


\vskip+1ex
\noindent \dotfill
\vskip+1ex
\bibliographystyle{mdpi}
\bibliography{Reference}

\end{document}