\documentclass[journal,onecolumn,12pt]{IEEEtran}

\usepackage{amsmath,amssymb,bm}
\usepackage{amsthm, amsfonts}
\usepackage{bm,bbm}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{fancybox}
\usepackage{url,booktabs}
\usepackage[round]{natbib}

\usepackage{xr}



\title{Reply to Reviewer's Comments on\\
``Training a gaming agent on brainwaves''}
\author{}

\begin{document}

\maketitle
\pagenumbering{roman}
\setcounter{page}{1}

We appreciate a lot all the comments and feedback provided by the AE and the Reviewers.

In the following, we discuss all the changes with each raised issue.

\vskip+1ex
\noindent \dotfill

\section*{\fbox{AE Transcript:}}

AE's Comments to Author:

Associate Editor
Comments to the Author:
The paper is improved but would benefit from a further update to
- improve abstract
- provide further interpretation/discussion of findings
- fix minor errors/typos
- address the grand average recommendation

\section*{\fbox{Reviewer \#1 Transcript:}}

Comments to the Author
The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

Training a Gaming Agent on Brainwaves
The paper has been improved. IMO, there are still areas that need further attention to improve the paper further.
Page 1
The abstract still reads like an introduction until the last sentence.  I would begin with sentence: “Error-related potential (ErrP) are a particular type of ERP that can be elicited by a person who attends a recognizable error.”

Expand the results of the paper as this is the contribution.
“Results show that there is an  \underline{effective} transfer of information and that the agent learns \underline{successfully} to solve the game efficiently.
Both the underlined terms need to cite evidence from the paper.


Line 40 “This information is used to make a gaming agent $<$that$>$ improves ....”


Line 11 col 2 “how biological agents learn from its $<$their$>$ environment by exploring it and getting feedback rewards, either negative or positive.”


Line 17 col2 "\sout{Nonneglected} is the influence of DeepBrain’s AlphaGo project,” // Re-word


Line 24 col 2 “The papers [8], [9], [10] have successfully demonstrated that a robot can be controlled by obtaining a reward signal from a person’s brain activity, \sout{which} who is observing the robot, $<$to$>$ solve a task.”


Line 58 Put in a reference number from ethics committee from which approval was sough (e.g. University committee)


Page 2
Line 56 “Data \sout{is} $<$are$>$ handled and processed with the OpenVibe Designer,” // datum-data


Line 37 col 2 “This dataset has been published on the IEEE DataPort initiative [15].” // I would move this to the end of next paragraph (or possibly end of paper)


Page 4
Lin 24 This allows \sout{to learn} the Q-Table $<$to learn$>$ based on the subject’s feedback from the movements the agent
took, which are chosen pseudo-randomly, while executing the brainwave session.


Line 50 “The best overall performance is obtained using Logistic Regression.” // also need to discuss the significance of the overall levels of accuracy in Discussion. Is a best performance of 0.672 useful/acceptable? How does it compare to other researchers (if a comparison is possible).


Line 42 col 2 “These results are also consistent with their classification ROC curves, shown \sout{on} $<$in$>$ Figures 6 obtained for both subjects, where the area under the curve are close to chance level.” // Why didn’t you show one good ROC curve and one bad one. This would provide the basis for comparison.


Page 5
Figure 5 “Y axis shows the averaged number of steps, while x axis show the number of
experiences used to cumulative train the Q-Table.”
// Why does Fig5 E only have 2 sets of data? Some discussion of the (smaller) change from 1 – 2 would be appropriate (e.g. I Discussion)
Line 51 “No\sout{t} performance gain is evidenced, the agents learn nothing which implies that the reward information is useless.” // useless is not a good choice of wording –provides no value


Line 55 “It can be seen that the overall performance of the agent improves as long as there are more experiences to be used to train it, regardless if they were generated from the brainwaves classification from different subjects.” // more explanation/discussion needed. Is this an average effect from positive learners?


This work aims to state whether ErrP signals could be used to train a gaming agent using reinforcement learning. The collected data show that ErrP signals can in fact be classified and used to train an agent effectively. // Can you link these findings to confirm/challenge other recent research in Err potentials


Page 6
Fig 8 – why use data from subject 6 (a non-learner)?


Line 42 “However, even though this implies that the agent misses frequently that an action taken is wrong, this is not hindering the overall performance and the agent is still learning” //


Line 20 col 2 Despite that, the rewards generated from different subjects can be used to train the same Q- Table to improve its performance, which may lead to strategies where the overall performance is improved based on the information from different human critics at the same time.
//These are key findings, worth more discussion/interpretation and inclusion in abstract.



\section*{\fbox{Reviewer \#2 Transcript:}}

Comments to the Author
The authors have satisfactorily addressed my concerns. I have a minor comment regarding Figure 3. It is unclear what the brown bar labelled GrandAvg represents since there is one for each participant. Please clarify. I suggest having another set of bars (a ninth) for the grand average (average over participants) for each of the algorithms used.


\subsection*{\ovalbox{AE General Comments}}
AE's Comments to Author:

Associate Editor
Comments to the Author:
The paper is improved but would benefit from a further update to
- improve abstract

\begin{quotation}
{\color{blue}
We rewrote the abstract explaining in further details the obtained results.  
}
\end{quotation}


- provide further interpretation/discussion of findings

\begin{quotation}
{\color{blue}
We included extra information and verified the contributions/findings.  The abstract was modified as well as the Conclusion (Section IV).
}
\end{quotation}

- fix minor errors/typos

\begin{quotation}
{\color{blue}
We fixed the carefully reported issues gently provided by Reviewers.
}
\end{quotation}

- address the grand average recommendation

\begin{quotation}
{\color{blue}
We addressed this issue.  We think there could be a misunderstanding from our side about the suggestion/recommendation of the Reviewer.  We added additional information in this response letter, and apologize for this.
}
\end{quotation}

\subsection*{\ovalbox{Reviewer 1 General Comments}}

Comments to the Author
The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

Training a Gaming Agent on Brainwaves
The paper has been improved. IMO, there are still areas that need further attention to improve the paper further.
Page 1
The abstract still reads like an introduction until the last sentence.  I would begin with sentence: “Error-related potential (ErrP) are a particular type of ERP that can be elicited by a person who attends a recognizable error.”

\begin{quotation}
{\color{blue}
We started the abstract as suggested, and rewrote it detailing the obtained results.   Thank you very much for your suggestion.
}
\end{quotation}


Expand the results of the paper as this is the contribution.
“Results show that there is an  \underline{effective} transfer of information and that the agent learns \underline{successfully} to solve the game efficiently.
Both the underlined terms need to cite evidence from the paper.

\begin{quotation}
{\color{blue}
We included the following information in the abstract and added it in the Conclusions Section (IV), showing evidence from the paper.

Each time the gaming agent plays this simple game, it takes on average around 100 steps to reach the target spot.  We trained a classifier to recognize Error Potential from observational human critics that watch the agent playing the game.  We let the agent play again and we mark movements that trigger an error potential from the human critic.  We used those movements as rewards in a Reinforcement Learning scheme, and use them to train a Q-Table.  When we let the agent plays the game again, the number of steps that requires to solve the game is now reduced.  If we provide feedback based on random signals, we verified that no reduction is achieved and the average number of steps does not change.   This shows that there is an effective transfer of information from the brainwaves to the agent.   As this process is repeated, the agent keeps improving solving the game effectively, i.e. performing the minimum number of required steps to reach the goal.

}
\end{quotation}

Line 40 “This information is used to make a gaming agent $<$that$>$ improves ....”

\begin{quotation}
{\color{blue}
This text is no longer contained in the manuscript.  We verified throughout the Manuscript to avoid similar mistakes.
}
\end{quotation}

Line 11 col 2 “how biological agents learn from its $<$their$>$ environment by exploring it and getting feedback rewards, either negative or positive.”

\begin{quotation}
{\color{blue}
This text is no longer contained in the manuscript.  We verified throughout the Manuscript to avoid similar mistakes.
}
\end{quotation}

Line 17 col2 "\sout{Nonneglected} is the influence of DeepBrain’s AlphaGo project,” // Re-word

\begin{quotation}
{\color{blue}
This was already solved in a previous version, and the manuscript no longer contains this line.  We appreciate a lot for your suggestion.
}
\end{quotation}

Line 24 col 2 “The papers [8], [9], [10] have successfully demonstrated that a robot can be controlled by obtaining a reward signal from a person’s brain activity, \sout{which} who is observing the robot, $<$to$>$ solve a task.”

\begin{quotation}
{\color{blue}
Fixed issue.
}
\end{quotation}

Line 58 Put in a reference number from ethics committee from which approval was sough (e.g. University committee)

\begin{quotation}
{\color{blue}
We added the missing information on Section II.A.  Thanks for pointing out this important issue.
}
\end{quotation}

Page 2
Line 56 “Data \sout{is} $<$are$>$ handled and processed with the OpenVibe Designer,” // datum-data

\begin{quotation}
{\color{blue}
Fixed issue for every appearances of \textbf{data}.
}
\end{quotation}

Line 37 col 2 “This dataset has been published on the IEEE DataPort initiative [15].” // I would move this to the end of next paragraph (or possibly end of paper)

\begin{quotation}
{\color{blue}
Added information in the form of a footnote on Section II.B.
}
\end{quotation}

Page 4
Lin 24 This allows \sout{to learn} the Q-Table $<$to learn$>$ based on the subject’s feedback from the movements the agent
took, which are chosen pseudo-randomly, while executing the brainwave session.

\begin{quotation}
{\color{blue}
Fixed issue.  We appreciate a lot the level of detail in the comments.
}
\end{quotation}

Line 50 “The best overall performance is obtained using Logistic Regression.” // also need to discuss the significance of the overall levels of accuracy in Discussion. Is a best performance of 0.672 useful/acceptable? How does it compare to other researchers (if a comparison is possible).

\begin{quotation}
{\color{blue}
On Section IV, third paragraph we tackled this issue.  The overall performance of 0.672 is low.  Although a comparison can be misleading in the context of the ErrP experiment, we added a brief comparison to other similar works.  We also emphasized this point in future  works at the end of the Conclusions section.
}
\end{quotation}

Line 42 col 2 “These results are also consistent with their classification ROC curves, shown \sout{on} $<$in$>$ Figures 6 obtained for both subjects, where the area under the curve are close to chance level.” // Why didn’t you show one good ROC curve and one bad one. This would provide the basis for comparison.

\begin{quotation}
{\color{blue}
Due to previous Reviewer's requests, we added all the ROC curves on the new Figure 7.  We now emphasized on this figure's caption which ROC curves where showing an effective identification of the ErrP potential and which did not.  We added this in the text emphasizing the important finding that we could not provide an enhancement of the agent performance based on the rewards from subjects where the obtained ROC curves were not good.
}
\end{quotation}

Page 5
Figure 5 “Y axis shows the averaged number of steps, while x axis show the number of
experiences used to cumulative train the Q-Table.”
// Why does Fig5 E only have 2 sets of data? Some discussion of the (smaller) change from 1 – 2 would be appropriate (e.g. I Discussion)
Line 51 “No\sout{t} performance gain is evidenced, the agents learn nothing which implies that the reward information is useless.” // useless is not a good choice of wording –provides no value

\begin{quotation}
{\color{blue}
We fixed the inappropriate wording, and appreciate the Reviewer for pointing this out.  
Figure 5, for Subject 5 (in the first version of the manuscript it was labeled E) has only two sets of data because that Participant (OHC) couldn't complete more sessions.  Hence we used two training sessions to calculate the Q-Table and subsequently performed the run session (i.e. the agent performs movements randomly based on the trained QTable for 200 iterations).
}
\end{quotation}

Line 55 “It can be seen that the overall performance of the agent improves as long as there are more experiences to be used to train it, regardless if they were generated from the brainwaves classification from different subjects.” // more explanation/discussion needed. Is this an average effect from positive learners?

\begin{quotation}
{\color{blue}
This is a great question.  Traditional Transfer Learning strategies focus on the procedure to enrich a classifier training it with a vast dataset and using it to generalize to new data or unseen scenarios.  While deaing with EEG data, this strategy tends to fail, and other approaches are required.  We found in this work, that even when that strategy fails, if the system is coupled with a basic reinforcement learning algorithm, the cumulative rewards from different subjects, can be used to effectively transmit information and improve the agent performance.  We agree with the Reviewer that this may be due to the average effect of positive or learners with a higher accuracy, but we think it may be related to the fact that the RL algorithm is quite robust and if we can get a higher specificity it will lead to eventually learn, and the usage of various OHCs help to increase the available data to do so.
}
\end{quotation}

This work aims to state whether ErrP signals could be used to train a gaming agent using reinforcement learning. The collected data show that ErrP signals can in fact be classified and used to train an agent effectively. // Can you link these findings to confirm/challenge other recent research in Err potentials

\begin{quotation}
{\color{blue}
We believe this result is quite supported in the literature.  You can check here here here here here.  Our novel approach is to use the rewards to train an agent that can perform on its own after the training has been performed, and the game mechanics to verify the flow of information.  This information was added in the Conclusion Section IV.
}
\end{quotation}

Page 6
Fig 8 – why use data from subject 6 (a non-learner)?

\begin{quotation}
{\color{blue}
Subject 5 and 6 aren't used to train the algorithm in this instance. This information was emphasized on the Figure's caption and on the main text in the Result Section.
}
\end{quotation}

Line 42 “However, even though this implies that the agent misses frequently that an action taken is wrong, this is not hindering the overall performance and the agent is still learning” //

Line 20 col 2 Despite that, the rewards generated from different subjects can be used to train the same Q- Table to improve its performance, which may lead to strategies where the overall performance is improved based on the information from different human critics at the same time.
//These are key findings, worth more discussion/interpretation and inclusion in abstract.

\begin{quotation}
{\color{blue}
We added it because is a very interesting results to overcome issues in transfer learning while dealing with biological data.  We appreciate the reviewer for this very important point.
}
\end{quotation}


\subsection*{\ovalbox{Reviewer 2 General Comments}}

Comments to the Author

The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

The authors have satisfactorily addressed my concerns. I have a minor comment regarding Figure 3. It is unclear what the brown bar labelled GrandAvg represents since there is one for each participant. Please clarify. I suggest having another set of bars (a ninth) for the grand average (average over participants) for each of the algorithms used.

\begin{quotation}
{\color{blue}
We think we misunderstood previous Reviewer's suggestions.  We added the brown bar on the previous Figure 3 (now Figure 5) because we assumed the Reviewer was referring to perform a classification using epoched averaged signal segments.  We used 5-segments to obtain an averaged signal with an enhanced signal-to-noise ratio (Wim van Drongelen, 4 - Signal Averaging, Editor(s): Signal Processing for Neuroscientists, Academic Press, 2007, Pages 55-70,) per Participant, and performed the classification of ErrP components based on those averaged signals for each Participant.  We used only Logistic Regression as classification algorithm, as it was the one with which we achieved better performance for single signal segments.   It is clear now that the brown bars were confusing and we have decided to remove them.  
Instead, we have now included a new Figure 3, where we plotted the \textbf{Grand Average} signals over all the participants for the two experimental conditions.  The top subfigure shows the signals for the "move closer to the target" condition, whereas the bottom subfigure represents the grand average for the "move further" condition.  
On the other hand, Figure 9 shows the results of using information to train a classifier with data from one Participant and use it to identify rewards for another participant.  This is performed in a single-trial approach from individual segments without any signal averaging.  
We apologize to the Reviewer for our misunderstanding and ask back for more information if it feels like the misunderstanding persisted.
}
\end{quotation}


\vskip+1ex
\noindent \dotfill
\vskip+1ex
\bibliographystyle{IEEEtran}
%\bibliography{Reference}

\end{document}
