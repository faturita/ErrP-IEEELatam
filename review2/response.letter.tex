\documentclass[journal,onecolumn,12pt]{IEEEtran}

\usepackage{amsmath,amssymb,bm}
\usepackage{amsthm, amsfonts}
\usepackage{bm,bbm}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{fancybox}
\usepackage{url,booktabs}
\usepackage[round]{natbib}

\usepackage{xr}



\title{Reply to Reviewer's Comments on\\
``Training a gaming agent on brainwaves''}
\author{}

\begin{document}

\maketitle
\pagenumbering{roman}
\setcounter{page}{1}

We appreciate a lot all the comments and feedback provided by the AE and the Reviewers.

In the following, we discuss all the changes with each raised issue.

\vskip+1ex
\noindent \dotfill

\section*{\fbox{AE Transcript:}}

AE's Comments to Author:

Associate Editor
Comments to the Author:
The paper is improved but would benefit from a further update to
- improve abstract
- provide further interpretation/discussion of findings
- fix minor errors/typos
- address the grand average recommendation

\section*{\fbox{Reviewer \#1 Transcript:}}

Comments to the Author
The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

Training a Gaming Agent on Brainwaves
The paper has been improved. IMO, there are still areas that need further attention to improve the paper further.
Page 1
The abstract still reads like an introduction until the last sentence.  I would begin with sentence: “Error-related potential (ErrP) are a particular type of ERP that can be elicited by a person who attends a recognizable error.”

Expand the results of the paper as this is the contribution.
“Results show that there is an  \underline{effective} transfer of information and that the agent learns \underline{successfully} to solve the game efficiently.
Both the underlined terms need to cite evidence from the paper.


Line 40 “This information is used to make a gaming agent $<$that$>$ improves ....”


Line 11 col 2 “how biological agents learn from its $<$their$>$ environment by exploring it and getting feedback rewards, either negative or positive.”


Line 17 col2 "\sout{Nonneglected} is the influence of DeepBrain’s AlphaGo project,” // Re-word


Line 24 col 2 “The papers [8], [9], [10] have successfully demonstrated that a robot can be controlled by obtaining a reward signal from a person’s brain activity, \sout{which} who is observing the robot, $<$to$>$ solve a task.”


Line 58 Put in a reference number from ethics committee from which approval was sough (e.g. University committee)


Page 2
Line 56 “Data \sout{is} $<$are$>$ handled and processed with the OpenVibe Designer,” // datum-data


Line 37 col 2 “This dataset has been published on the IEEE DataPort initiative [15].” // I would move this to the end of next paragraph (or possibly end of paper)


Page 4
Lin 24 This allows \sout{to learn} the Q-Table $<$to learn$>$ based on the subject’s feedback from the movements the agent
took, which are chosen pseudo-randomly, while executing the brainwave session.


Line 50 “The best overall performance is obtained using Logistic Regression.” // also need to discuss the significance of the overall levels of accuracy in Discussion. Is a best performance of 0.672 useful/acceptable? How does it compare to other researchers (if a comparison is possible).


Line 42 col 2 “These results are also consistent with their classification ROC curves, shown \sout{on} $<$in$>$ Figures 6 obtained for both subjects, where the area under the curve are close to chance level.” // Why didn’t you show one good ROC curve and one bad one. This would provide the basis for comparison.


Page 5
Figure 5 “Y axis shows the averaged number of steps, while x axis show the number of
experiences used to cumulative train the Q-Table.”
// Why does Fig5 E only have 2 sets of data? Some discussion of the (smaller) change from 1 – 2 would be appropriate (e.g. I Discussion)
Line 51 “No\sout{t} performance gain is evidenced, the agents learn nothing which implies that the reward information is useless.” // useless is not a good choice of wording –provides no value


Line 55 “It can be seen that the overall performance of the agent improves as long as there are more experiences to be used to train it, regardless if they were generated from the brainwaves classification from different subjects.” // more explanation/discussion needed. Is this an average effect from positive learners?


This work aims to state whether ErrP signals could be used to train a gaming agent using reinforcement learning. The collected data show that ErrP signals can in fact be classified and used to train an agent effectively. // Can you link these findings to confirm/challenge other recent research in Err potentials


Page 6
Fig 8 – why use data from subject 6 (a non-learner)?


Line 42 “However, even though this implies that the agent misses frequently that an action taken is wrong, this is not hindering the overall performance and the agent is still learning” //


Line 20 col 2 Despite that, the rewards generated from different subjects can be used to train the same Q- Table to improve its performance, which may lead to strategies where the overall performance is improved based on the information from different human critics at the same time.
//These are key findings, worth more discussion/interpretation and inclusion in abstract.



\section*{\fbox{Reviewer \#2 Transcript:}}

Comments to the Author
The authors have satisfactorily addressed my concerns. I have a minor comment regarding Figure 3. It is unclear what the brown bar labelled GrandAvg represents since there is one for each participant. Please clarify. I suggest having another set of bars (a ninth) for the grand average (average over participants) for each of the algorithms used.


\subsection*{\ovalbox{AE General Comments}}
AE's Comments to Author:

Associate Editor
Comments to the Author:
The paper is improved but would benefit from a further update to
- improve abstract

\begin{quotation}
{\color{blue}
Rewrote the abstract talking in further detail obtained results.
}
\end{quotation}


- provide further interpretation/discussion of findings

\begin{quotation}
{\color{blue}
BLA BLA BLA
}
\end{quotation}

- fix minor errors/typos

\begin{quotation}
{\color{blue}
BLA BLA BLA
}
\end{quotation}

- address the grand average recommendation

\begin{quotation}
{\color{blue}
BLA BLA BLA
}
\end{quotation}

\subsection*{\ovalbox{Reviewer 1 General Comments}}

Comments to the Author
The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

Training a Gaming Agent on Brainwaves
The paper has been improved. IMO, there are still areas that need further attention to improve the paper further.
Page 1
The abstract still reads like an introduction until the last sentence.  I would begin with sentence: “Error-related potential (ErrP) are a particular type of ERP that can be elicited by a person who attends a recognizable error.”

\begin{quotation}
{\color{blue}
We started the abstract as suggested, thank you very much for it.  We rewrote the abstract detailing the obtained results.
}
\end{quotation}


Expand the results of the paper as this is the contribution.
“Results show that there is an  \underline{effective} transfer of information and that the agent learns \underline{successfully} to solve the game efficiently.
Both the underlined terms need to cite evidence from the paper.

\begin{quotation}
{\color{blue}
We included this information in the abstract and added it in the Conclusions Section (IV).
}
\end{quotation}

Line 40 “This information is used to make a gaming agent $<$that$>$ improves ....”

\begin{quotation}
{\color{blue}
This text is no longer contained in the manuscript.  We verified along the Manuscript to avoid similar mistakes.
}
\end{quotation}

Line 11 col 2 “how biological agents learn from its $<$their$>$ environment by exploring it and getting feedback rewards, either negative or positive.”

\begin{quotation}
{\color{blue}
This text is no longer contained in the manuscript.  We verified along the Manuscript to avoid similar mistakes.
}
\end{quotation}

Line 17 col2 "\sout{Nonneglected} is the influence of DeepBrain’s AlphaGo project,” // Re-word

\begin{quotation}
{\color{blue}
This was already solved in a previous version, and the manuscript no longer contains this line.  We appreciate a lot for your suggestion.
}
\end{quotation}

Line 24 col 2 “The papers [8], [9], [10] have successfully demonstrated that a robot can be controlled by obtaining a reward signal from a person’s brain activity, \sout{which} who is observing the robot, $<$to$>$ solve a task.”

\begin{quotation}
{\color{blue}
Fixed issue.
}
\end{quotation}

Line 58 Put in a reference number from ethics committee from which approval was sough (e.g. University committee)

\begin{quotation}
{\color{blue}
We added the missing information on Section II.A.  Thanks for pointing out this important issue.
}
\end{quotation}

Page 2
Line 56 “Data \sout{is} $<$are$>$ handled and processed with the OpenVibe Designer,” // datum-data

\begin{quotation}
{\color{blue}
Fixed issue for every appearances of \textbf{data is}.
}
\end{quotation}

Line 37 col 2 “This dataset has been published on the IEEE DataPort initiative [15].” // I would move this to the end of next paragraph (or possibly end of paper)

\begin{quotation}
{\color{blue}
Added information in the form of a footnote on Section II.B.
}
\end{quotation}

Page 4
Lin 24 This allows \sout{to learn} the Q-Table $<$to learn$>$ based on the subject’s feedback from the movements the agent
took, which are chosen pseudo-randomly, while executing the brainwave session.

\begin{quotation}
{\color{blue}
Fixed issue.  We appreciate a lot the level of detail in the comments.
}
\end{quotation}

Line 50 “The best overall performance is obtained using Logistic Regression.” // also need to discuss the significance of the overall levels of accuracy in Discussion. Is a best performance of 0.672 useful/acceptable? How does it compare to other researchers (if a comparison is possible).

\begin{quotation}
{\color{blue}
On Section IV, third paragraph we tackled this issue.  The overall performance of 0.672 is low.  Although a comparison can be misleading in the context of the ErrP experiment, we added a brief comparison to other similar works.  We also emphasized this point in future works where there is room for improvement in terms of the classification and processing pipeline.
}
\end{quotation}

Line 42 col 2 “These results are also consistent with their classification ROC curves, shown \sout{on} $<$in$>$ Figures 6 obtained for both subjects, where the area under the curve are close to chance level.” // Why didn’t you show one good ROC curve and one bad one. This would provide the basis for comparison.

\begin{quotation}
{\color{blue}
We emphasize on the Figure 6 caption which ROC curves where good which were not so good.  We added this in the text and the important finding that we could not provide an enhencement of the agent for those subjects where the obtained ROC curves were not good.
}
\end{quotation}

Page 5
Figure 5 “Y axis shows the averaged number of steps, while x axis show the number of
experiences used to cumulative train the Q-Table.”
// Why does Fig5 E only have 2 sets of data? Some discussion of the (smaller) change from 1 – 2 would be appropriate (e.g. I Discussion)
Line 51 “No\sout{t} performance gain is evidenced, the agents learn nothing which implies that the reward information is useless.” // useless is not a good choice of wording –provides no value

\begin{quotation}
{\color{blue}
We fixed the inappropriate wording, and appreciate the Reviewer for pointing this out.  
Figure 5, for Subject 5 (in the first version of the manuscript it was labeled E) has only two sets of data because that Participant (OHC) couldn't complete more sessions.  Hence we used two training sessions to calculate the Q-Table and subsequently performed the run session (i.e. the agent performs movements randomly based on the trained QTable for 200 iterations).
}
\end{quotation}

Line 55 “It can be seen that the overall performance of the agent improves as long as there are more experiences to be used to train it, regardless if they were generated from the brainwaves classification from different subjects.” // more explanation/discussion needed. Is this an average effect from positive learners?

\begin{quotation}
{\color{blue}
This is a great question.  Traditional Transfer Learning strategies focus on the procedure to enrich a classifier training it with a vast dataset and using it to generalize to new data or unseen scenarios.  While deaing with EEG data, this strategy tends to fail, and other approaches are required.  We found in this work, that even when that strategy fails, if the system is coupled with a basic reinforcement learning algorithm, the cumulative rewards from different subjects, can be used to effectively transmit information and improve the agent performance.  We agree with the Reviewer that this may be due to the average effect of positive or learners with a higher accuracy, but we think it may be related to the fact that the RL algorithm is quite robust and if we can get a higher specificity it will lead to eventually learn, and the usage of various OHCs help to increase the available data to do so.
}
\end{quotation}

This work aims to state whether ErrP signals could be used to train a gaming agent using reinforcement learning. The collected data show that ErrP signals can in fact be classified and used to train an agent effectively. // Can you link these findings to confirm/challenge other recent research in Err potentials

\begin{quotation}
{\color{blue}
We believe this result is quite supported in the literature.  You can check here here here here here.  Our novel approach is to use the rewards to train an agent that can perform on its own after the training has been performed, and the game mechanics to verify the flow of information.  This information was added in the Conclusion Section IV.
}
\end{quotation}

Page 6
Fig 8 – why use data from subject 6 (a non-learner)?

\begin{quotation}
{\color{blue}
Subject 5 and 6 aren't used to train the algorithm in this instance. This information was emphasized on the Figure's caption and on the main text in the Result Section.
}
\end{quotation}

Line 42 “However, even though this implies that the agent misses frequently that an action taken is wrong, this is not hindering the overall performance and the agent is still learning” //

Line 20 col 2 Despite that, the rewards generated from different subjects can be used to train the same Q- Table to improve its performance, which may lead to strategies where the overall performance is improved based on the information from different human critics at the same time.
//These are key findings, worth more discussion/interpretation and inclusion in abstract.

\begin{quotation}
{\color{blue}
We added it because is a very interesting results to overcome issues in transfer learning while dealing with biological data.  We appreciate the reviewer for this very important point.
}
\end{quotation}


\subsection*{\ovalbox{Reviewer 2 General Comments}}

Comments to the Author

The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

The authors have satisfactorily addressed my concerns. I have a minor comment regarding Figure 3. It is unclear what the brown bar labelled GrandAvg represents since there is one for each participant. Please clarify. I suggest having another set of bars (a ninth) for the grand average (average over participants) for each of the algorithms used.

\begin{quotation}
{\color{blue}
fdsjlfjldksj
}
\end{quotation}


\vskip+1ex
\noindent \dotfill
\vskip+1ex
\bibliographystyle{IEEEtran}
\bibliography{Reference}

\end{document}
