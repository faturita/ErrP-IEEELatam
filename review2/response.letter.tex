\documentclass[journal,onecolumn,12pt]{IEEEtran}

\usepackage{amsmath,amssymb,bm}
\usepackage{amsthm, amsfonts}
\usepackage{bm,bbm}
\usepackage[normalem]{ulem}
\usepackage{color}
\usepackage{fancybox}
\usepackage{url,booktabs}
\usepackage[round]{natbib}

\usepackage{xr}



\title{Reply to Reviewer's Comments on\\
``Training a gaming agent on brainwaves''}
\author{}

\begin{document}

\maketitle
\pagenumbering{roman}
\setcounter{page}{1}

We appreciate a lot all the comments and feedback provided by the AE and the Reviewers.

In the following, we discuss all the changes with each raised issue.

\vskip+1ex
\noindent \dotfill

\section*{\fbox{AE Transcript:}}

AE's Comments to Author:

Associate Editor
Comments to the Author:
The paper is improved but would benefit from a further update to
- improve abstract
- provide further interpretation/discussion of findings
- fix minor errors/typos
- address the grand average recommendation

\section*{\fbox{Reviewer \#1 Transcript:}}

Comments to the Author
The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

Training a Gaming Agent on Brainwaves
The paper has been improved. IMO, there are still areas that need further attention to improve the paper further.
Page 1
The abstract still reads like an introduction until the last sentence.  I would begin with sentence: “Error-related potential (ErrP) are a particular type of ERP that can be elicited by a person who attends a recognizable error.”

Expand the results of the paper as this is the contribution.
“Results show that there is an  \underline{effective} transfer of information and that the agent learns \underline{successfully} to solve the game efficiently.
Both the underlined terms need to cite evidence from the paper.


Line 40 “This information is used to make a gaming agent $<$that$>$ improves ....”


Line 11 col 2 “how biological agents learn from its $<$their$>$ environment by exploring it and getting feedback rewards, either negative or positive.”


Line 17 col2 "\sout{Nonneglected} is the influence of DeepBrain’s AlphaGo project,” // Re-word


Line 24 col 2 “The papers [8], [9], [10] have successfully demonstrated that a robot can be controlled by obtaining a reward signal from a person’s brain activity, \sout{which} who is observing the robot, $<$to$>$ solve a task.”


Line 58 Put in a reference number from ethics committee from which approval was sough (e.g. University committee)


Page 2
Line 56 “Data \sout{is} $<$are$>$ handled and processed with the OpenVibe Designer,” // datum-data


Line 37 col 2 “This dataset has been published on the IEEE DataPort initiative [15].” // I would move this to the end of next paragraph (or possibly end of paper)


Page 4
Lin 24 This allows \sout{to learn} the Q-Table $<$to learn$>$ based on the subject’s feedback from the movements the agent
took, which are chosen pseudo-randomly, while executing the brainwave session.


Line 50 “The best overall performance is obtained using Logistic Regression.” // also need to discuss the significance of the overall levels of accuracy in Discussion. Is a best performance of 0.672 useful/acceptable? How does it compare to other researchers (if a comparison is possible).


Line 42 col 2 “These results are also consistent with their classification ROC curves, shown \sout{on} $<$in$>$ Figures 6 obtained for both subjects, where the area under the curve are close to chance level.” // Why didn’t you show one good ROC curve and one bad one. This would provide the basis for comparison.


Page 5
Figure 5 “Y axis shows the averaged number of steps, while x axis show the number of
experiences used to cumulative train the Q-Table.”
// Why does Fig5 E only have 2 sets of data? Some discussion of the (smaller) change from 1 – 2 would be appropriate (e.g. I Discussion)
Line 51 “No\sout{t} performance gain is evidenced, the agents learn nothing which implies that the reward information is useless.” // useless is not a good choice of wording –provides no value


Line 55 “It can be seen that the overall performance of the agent improves as long as there are more experiences to be used to train it, regardless if they were generated from the brainwaves classification from different subjects.” // more explanation/discussion needed. Is this an average effect from positive learners?


This work aims to state whether ErrP signals could be used to train a gaming agent using reinforcement learning. The collected data show that ErrP signals can in fact be classified and used to train an agent effectively. // Can you link these findings to confirm/challenge other recent research in Err potentials


Page 6
Fig 8 – why use data from subject 6 (a non-learner)?


Line 42 “However, even though this implies that the agent misses frequently that an action taken is wrong, this is not hindering the overall performance and the agent is still learning” //


Line 20 col 2 Despite that, the rewards generated from different subjects can be used to train the same Q- Table to improve its performance, which may lead to strategies where the overall performance is improved based on the information from different human critics at the same time.
//These are key findings, worth more discussion/interpretation and inclusion in abstract.



\section*{\fbox{Reviewer \#2 Transcript:}}

Comments to the Author
The authors have satisfactorily addressed my concerns. I have a minor comment regarding Figure 3. It is unclear what the brown bar labelled GrandAvg represents since there is one for each participant. Please clarify. I suggest having another set of bars (a ninth) for the grand average (average over participants) for each of the algorithms used.


\subsection*{\ovalbox{AE General Comments}}
AE's Comments to Author:

Associate Editor
Comments to the Author:
The paper is improved but would benefit from a further update to
- improve abstract

\begin{quotation}
{\color{blue}
Rewrote the abstract talking in further detail obtained results.
}
\end{quotation}


- provide further interpretation/discussion of findings

\begin{quotation}
{\color{blue}
BLA BLA BLA
}
\end{quotation}

- fix minor errors/typos

\begin{quotation}
{\color{blue}
BLA BLA BLA
}
\end{quotation}

- address the grand average recommendation

\begin{quotation}
{\color{blue}
BLA BLA BLA
}
\end{quotation}

\subsection*{\ovalbox{Reviewer 1 General Comments}}

Comments to the Author
The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

Training a Gaming Agent on Brainwaves
The paper has been improved. IMO, there are still areas that need further attention to improve the paper further.
Page 1
The abstract still reads like an introduction until the last sentence.  I would begin with sentence: “Error-related potential (ErrP) are a particular type of ERP that can be elicited by a person who attends a recognizable error.”

\begin{quotation}
{\color{blue}
Rewrote the abstract talking in further detail obtained results.
}
\end{quotation}


Expand the results of the paper as this is the contribution.
“Results show that there is an  \underline{effective} transfer of information and that the agent learns \underline{successfully} to solve the game efficiently.
Both the underlined terms need to cite evidence from the paper.

\begin{quotation}
{\color{blue}
This was solved in a previous version.
}
\end{quotation}

Line 40 “This information is used to make a gaming agent $<$that$>$ improves ....”

\begin{quotation}
{\color{blue}
This was solved in a previous version.
}
\end{quotation}

Line 11 col 2 “how biological agents learn from its $<$their$>$ environment by exploring it and getting feedback rewards, either negative or positive.”

\begin{quotation}
{\color{blue}
This was solved in a previous version.
}
\end{quotation}

Line 17 col2 "\sout{Nonneglected} is the influence of DeepBrain’s AlphaGo project,” // Re-word

\begin{quotation}
{\color{blue}
This was solved in a previous version.
}
\end{quotation}

Line 24 col 2 “The papers [8], [9], [10] have successfully demonstrated that a robot can be controlled by obtaining a reward signal from a person’s brain activity, \sout{which} who is observing the robot, $<$to$>$ solve a task.”

\begin{quotation}
{\color{blue}
Fixed issue.
}
\end{quotation}

Line 58 Put in a reference number from ethics committee from which approval was sough (e.g. University committee)

\begin{quotation}
{\color{blue}
We added the missing information on Section II.A.  Thanks for pointing out this important issue.
}
\end{quotation}

Page 2
Line 56 “Data \sout{is} $<$are$>$ handled and processed with the OpenVibe Designer,” // datum-data

\begin{quotation}
{\color{blue}
Fixed issue for all appeareances of "data is".
}
\end{quotation}

Line 37 col 2 “This dataset has been published on the IEEE DataPort initiative [15].” // I would move this to the end of next paragraph (or possibly end of paper)

\begin{quotation}
{\color{blue}
Added information in the form of a footnote on Section II.B.
}
\end{quotation}

Page 4
Lin 24 This allows \sout{to learn} the Q-Table $<$to learn$>$ based on the subject’s feedback from the movements the agent
took, which are chosen pseudo-randomly, while executing the brainwave session.

\begin{quotation}
{\color{blue}
Fixed issue.
}
\end{quotation}

Line 50 “The best overall performance is obtained using Logistic Regression.” // also need to discuss the significance of the overall levels of accuracy in Discussion. Is a best performance of 0.672 useful/acceptable? How does it compare to other researchers (if a comparison is possible).

\begin{quotation}
{\color{blue}
On Section IV, third paragraph we tackled this issue.  The overall performance of 0.672 is low.  Although a comparison can be misleading in the context of the experiment, because it is not clear at which extent the nature of the experiment can elicit a different ErrP response which will inevitable produce different level of accuracies for detection.  A comparison was added.
}
\end{quotation}

Line 42 col 2 “These results are also consistent with their classification ROC curves, shown \sout{on} $<$in$>$ Figures 6 obtained for both subjects, where the area under the curve are close to chance level.” // Why didn’t you show one good ROC curve and one bad one. This would provide the basis for comparison.

\begin{quotation}
{\color{blue}
TODO: Arregle error gramatical pero no agregu'e la ROC mala.
}
\end{quotation}

Page 5
Figure 5 “Y axis shows the averaged number of steps, while x axis show the number of
experiences used to cumulative train the Q-Table.”
// Why does Fig5 E only have 2 sets of data? Some discussion of the (smaller) change from 1 – 2 would be appropriate (e.g. I Discussion)
Line 51 “No\sout{t} performance gain is evidenced, the agents learn nothing which implies that the reward information is useless.” // useless is not a good choice of wording –provides no value

\begin{quotation}
{\color{blue}
fdsjlfjldksj
}
\end{quotation}

Line 55 “It can be seen that the overall performance of the agent improves as long as there are more experiences to be used to train it, regardless if they were generated from the brainwaves classification from different subjects.” // more explanation/discussion needed. Is this an average effect from positive learners?

\begin{quotation}
{\color{blue}
fdsjlfjldksj
}
\end{quotation}

This work aims to state whether ErrP signals could be used to train a gaming agent using reinforcement learning. The collected data show that ErrP signals can in fact be classified and used to train an agent effectively. // Can you link these findings to confirm/challenge other recent research in Err potentials

\begin{quotation}
{\color{blue}
fdsjlfjldksj
}
\end{quotation}

Page 6
Fig 8 – why use data from subject 6 (a non-learner)?

\begin{quotation}
{\color{blue}
Subject 5 and 6 aren't used to train the algorithm in this instance. Added this information in the figure caption.
}
\end{quotation}

Line 42 “However, even though this implies that the agent misses frequently that an action taken is wrong, this is not hindering the overall performance and the agent is still learning” //

\begin{quotation}
{\color{blue}
fdsjlfjldksj
}
\end{quotation}

Line 20 col 2 Despite that, the rewards generated from different subjects can be used to train the same Q- Table to improve its performance, which may lead to strategies where the overall performance is improved based on the information from different human critics at the same time.
//These are key findings, worth more discussion/interpretation and inclusion in abstract.

\begin{quotation}
{\color{blue}
fdsjlfjldksj
}
\end{quotation}


\subsection*{\ovalbox{Reviewer 2 General Comments}}

Comments to the Author

The paper is improved. The contribution is clearer. Thank you for the reply to comments. I attach some further comments.

The authors have satisfactorily addressed my concerns. I have a minor comment regarding Figure 3. It is unclear what the brown bar labelled GrandAvg represents since there is one for each participant. Please clarify. I suggest having another set of bars (a ninth) for the grand average (average over participants) for each of the algorithms used.

\begin{quotation}
{\color{blue}
fdsjlfjldksj
}
\end{quotation}


\vskip+1ex
\noindent \dotfill
\vskip+1ex
\bibliographystyle{IEEEtran}
\bibliography{Reference}

\end{document}
